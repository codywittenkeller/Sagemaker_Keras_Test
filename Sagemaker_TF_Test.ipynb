{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL\n",
    "\n",
    "\n",
    "## for Deep-learing:\n",
    "import keras\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD \n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "import itertools\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# initiate session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "#define specific AWS Role\n",
    "role = 'arn:aws:iam::' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload the data to a S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = sagemaker_session.upload_data(path='data', key_prefix='data/TF_Test')\n",
    "\n",
    "model_artifacts_location = 's3://data/TF_Test/artifacts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy as np\r\n",
      "import os\r\n",
      "import tensorflow as tf\r\n",
      "from tensorflow.python.keras.layers import InputLayer, Conv2D, Activation, MaxPooling2D, Dropout, Flatten, Dense, LSTM\r\n",
      "from tensorflow.python.keras.models import Sequential\r\n",
      "from tensorflow.python.keras.optimizers import RMSprop\r\n",
      "from tensorflow.python.saved_model.signature_constants import PREDICT_INPUTS\r\n",
      "\r\n",
      "from tensorflow.python.estimator.export.export import build_raw_serving_input_receiver_fn\r\n",
      "from tensorflow.python.estimator.export.export_output import PredictOutput\r\n",
      "\r\n",
      "\r\n",
      "INPUT_TENSOR_NAME = \"PREDICT_INPUTS_input\" # needs to match the name of the first layer + \"_input\"\r\n",
      "\r\n",
      "\r\n",
      "def keras_model_fn(hyperparameters):\r\n",
      "    model = tf.keras.models.Sequential()\r\n",
      "    model.add(tf.keras.layers.LSTM(256, return_sequences=True,input_shape=(1,16), activation='relu', name='PREDICT_INPUTS'))\r\n",
      "    model.add(tf.keras.layers.Dropout(0.9))\r\n",
      "    model.add(tf.keras.layers.LSTM(256))\r\n",
      "    model.add(tf.keras.layers.Dropout(0.9))\r\n",
      "    model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\r\n",
      "    \r\n",
      "    _model = tf.keras.Model(inputs=model.input, outputs=model.output)\r\n",
      "    \r\n",
      "    _model.compile(loss='binary_crossentropy',\r\n",
      "                  optimizer='adam',\r\n",
      "                  metrics=['binary_accuracy'])\r\n",
      "    return _model\r\n",
      "\r\n",
      "def serving_input_fn(params):\r\n",
      "    tensor = tf.placeholder(tf.float32, shape=[1,1,16])\r\n",
      "    return build_raw_serving_input_receiver_fn({INPUT_TENSOR_NAME: tensor})()\r\n",
      "\r\n",
      "\r\n",
      "def train_input_fn(training_dir, params):\r\n",
      "    return _input_fn(training_dir, 'Train_Clean.csv')\r\n",
      "\r\n",
      "\r\n",
      "def eval_input_fn(training_dir, params):\r\n",
      "    return _input_fn(training_dir, 'Test_Clean.csv')\r\n",
      "\r\n",
      "\r\n",
      "def _input_fn(training_dir, training_filename):\r\n",
      "    training_set = tf.contrib.learn.datasets.base.load_csv_without_header(\r\n",
      "        filename=os.path.join(training_dir, training_filename), target_dtype=np.float32, features_dtype=np.float32)\r\n",
      "\r\n",
      "    return tf.estimator.inputs.numpy_input_fn(       \r\n",
      "        x={INPUT_TENSOR_NAME: np.array(training_set.data).reshape((-1,1,16))},\r\n",
      "        y=np.asarray(training_set.target).reshape((-1,1)),\r\n",
      "        num_epochs=None,\r\n",
      "        shuffle=True)()\r\n"
     ]
    }
   ],
   "source": [
    "# reveal referenced .py file needed for training\n",
    "!cat 'KERAS.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting script for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-786739926626\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-tensorflow-2018-11-30-00-06-57-506\n"
     ]
    },
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "TF_estimator = TensorFlow(entry_point='KERAS.py',\n",
    "                               role=role,\n",
    "                               output_path=model_artifacts_location,\n",
    "                               framework_version='1.10.0',\n",
    "                               training_steps= 100,                                  \n",
    "                               evaluation_steps= 100,\n",
    "                               hyperparameters={'learning_rate': 0.001},\n",
    "                               train_instance_count = 1,\n",
    "                               train_instance_type='ml.c4.4xlarge')\n",
    "\n",
    "TF_estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submiting a trained model for hosting\n",
    "\n",
    "The deploy() method creates an endpoint which serves prediction requests in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-tensorflow-2018-11-30-00-06-57-506\n",
      "WARNING:sagemaker:Using already existing model: sagemaker-tensorflow-2018-11-30-00-06-57-506\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-tensorflow-2018-11-30-00-06-57-506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "TF_predictor = TF_estimator.deploy(initial_instance_count=1, instance_type='ml.m4.4xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "prediction_set = tf.contrib.learn.datasets.base.load_csv_without_header(\n",
    "    filename=os.path.join('data/Test_Clean.csv'), target_dtype=np.float32, features_dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(prediction_set[0][0]).reshape((-1,1,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 16)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'outputs': {'dense': {'dtype': 1,\n",
       "   'tensor_shape': {'dim': [{'size': 1}, {'size': 1}]},\n",
       "   'float_val': [0.12401328980922699]}},\n",
       " 'model_spec': {'name': 'generic_model',\n",
       "  'version': {'value': 1543536652},\n",
       "  'signature_name': 'serving_default'}}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF_predictor.predict({'PREDICT_INPUTS_input':data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: \\n"
     ]
    }
   ],
   "source": [
    "sagemaker.Session().delete_endpoint(TF_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
